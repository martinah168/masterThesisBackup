{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 49]\n",
      "from_im shape: torch.Size([1, 1, 128, 96, 128])\n",
      "tesnor torch.Size([1, 1, 128, 96, 128])\n",
      "Model params: 1.27 M\n",
      "[-1.  1.]\n",
      "torch: torch.Size([1, 1, 128, 96, 128])\n",
      "mean: torch.Size([1, 1, 128, 96, 128])\n",
      "cond\n",
      "cond torch.Size([1, 512])\n",
      "stoch shape: torch.Size([1, 1, 128, 96, 128])\n",
      "shape: torch.Size([1, 1, 128, 96, 128])\n",
      "[-1.         -0.99999994 -0.9999998  ...  0.9999988   0.9999998\n",
      "  1.        ]\n",
      "[0.0000000e+00 2.9802322e-08 8.9406967e-08 ... 9.9999940e-01 9.9999988e-01\n",
      " 1.0000000e+00]\n",
      "[0. 1.]\n",
      "(128, 96, 128)\n",
      "[0. 1.]\n",
      "\u001b[0m[ ] 'set_array' with different dtype: from uint8 to float32\u001b[0m\u001b[0m\n",
      "\u001b[96m[*] Save Reconstructions/Reconstructed_ctfu00699_017_corpus_float_T200_T20.nii.gz as uint8\u001b[0m\u001b[0m\n",
      "[0 1]\n",
      "0.9964997499821416\n",
      "[ 0. 49.]\n",
      "[ 0. 49.]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pl_models.DEA import DAE_LitModel\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from BIDS import NII\n",
    "from BIDS.core.np_utils import np_dice\n",
    "import numpy as np\n",
    "from BIDS.core.np_utils import np_map_labels, Label_Map\n",
    "from sklearn.manifold import TSNE\n",
    "from dataloader.datasets.dataset_csv import model_map_to_segmentation_map, segmentation_map_to_model_map \n",
    "#3D\n",
    "def load_nifti(nii):\n",
    "        \n",
    "        from_im = nii.get_seg_array()\n",
    "        #range_print(torch.from_numpy(from_im.astype(float)),41,41)\n",
    "        #print(from_im[143,95,:])\n",
    "        print(np.unique(from_im))\n",
    "        from_im = np_map_labels(arr=from_im,label_map={50: 49})\n",
    "        from_im = from_im.astype(float)\n",
    "        from_im = np_map_labels(arr=from_im,label_map={49: 1})\n",
    "            \n",
    "        from_im = (from_im-0.5)/0.5\n",
    "        from_im = torch.from_numpy(from_im).unsqueeze(0).unsqueeze(0)\n",
    "        print(\"from_im shape:\", from_im.shape)\n",
    "        from_im = from_im.to(torch.float32)\n",
    "        \n",
    "        print(\"tesnor\", from_im.shape)\n",
    "        return from_im # , \"index\": target, \"cls_labels\": target}\n",
    "# def map_to_binary(y):\n",
    "#         #y = y.astype(float)\n",
    "#         labelmap = {i: 1 for i in range(41, 50)} #  labelmap = {i: round((i - 40)/9, ndigits=2) for i in range(41, 50)}\n",
    "#         #print(labelmap)\n",
    "#         return np_map_labels(y, labelmap)\n",
    "# def segmentation_map_to_model_map(y):\n",
    "#         y = y.astype(float)\n",
    "#         labelmap = {i: round((i - 40)/9, ndigits=5) for i in range(41, 50)} #  labelmap = {i: round((i - 40)/9, ndigits=2) for i in range(41, 50)}\n",
    "#         print(labelmap)\n",
    "#         return np_map_labels(y, labelmap)\n",
    "# def model_map_to_segmentation_map(x):\n",
    "#     x =(x+1)/2\n",
    "#     x *= 9\n",
    "#     x = np.round(x+40)# round nearest int\n",
    "#    # x = x+40\n",
    "#     x = np_map_labels(x, {40: 0})\n",
    "#     return x\n",
    "\n",
    "# def model_map_to_binary_map(x):\n",
    "#     x = np.round(x)\n",
    "#     return x\n",
    "\n",
    "#\"/media/DATA/martina_ma/cutout/verse040/verse040_023_subreg_cropped.nii.gz\",#\n",
    "#/media/DATA/martina_ma/cutout/tri177/tri177_007_subreg_cropped.nii.gz \n",
    "#in training set: /media/DATA/martina_ma/cutout/verse040/verse040_018_subreg_cropped.nii.gz\n",
    "#not in train and not in val: /media/DATA/martina_ma/cutout/verse040/verse040_023_subreg_cropped.nii.gz\n",
    "# Test sample /media/DATA/martina_ma/cutout/tri050/tri050_021_subreg_cropped.nii.gz\n",
    "# Train sample in old seg /media/DATA/martina_ma/cutout/verse529/verse529_014_new_subreg_cropped.nii.gz\n",
    "nii = NII.load(\n",
    "    \"/media/DATA/martina_ma/cutout_corpus/ctfu00699/ctfu00699_17_subreg_corpus.nii.gz\",\n",
    "    True,\n",
    ")\n",
    "from_im = load_nifti(nii)\n",
    "\n",
    "checkpoint_path =\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_corpus/version_20/checkpoints/epoch=25-step=197990d_score=1.0000_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_corpus/version_17/checkpoints/epoch=0-step=7615d_score=0.7215_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned/version_5/checkpoints/epoch=11-step=113760d_score=0.9303_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_95_old_verse_w_norm/version_11/checkpoints/epoch=0-step=1488_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_232/checkpoints/epoch=283-step=27190_latest.ckpt\"# \"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_133/checkpoints/epoch=3-step=310_latest.ckpt\"\n",
    "#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_135/checkpoints/epoch=27-step=2775_latest.ckpt\"#/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_133/checkpoints/epoch=3-step=310_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_118/checkpoints/epoch=80-step=243_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_116/checkpoints/epoch=81-step=246_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_108/checkpoints/epoch=47-step=144_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_105/checkpoints/epoch=69-step=210_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_63/checkpoints/epoch=79-step=240_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_60/checkpoints/epoch=71-step=216_latest.ckpt\"#lightning_logs/DAE_NAKO_256/version_26/checkpoints/epoch=404-step=405_latest.ckpt\"\n",
    "device = \"cuda:0\"  # or \"cpu\" if you want to use CPU\n",
    "assert Path(checkpoint_path).exists()\n",
    "model = DAE_LitModel.load_from_checkpoint(checkpoint_path)\n",
    "model.ema_model.eval()\n",
    "model.ema_model.to(device)\n",
    "\n",
    "img_data = from_im.to(device)\n",
    "print(np.unique(img_data[0][0].cpu().numpy()))\n",
    "\n",
    "print(\"torch:\",img_data.shape)\n",
    "# Ensure that the input has a single channel (if the model expects 1 channel)\n",
    "if img_data.shape[0] != 1:\n",
    "    raise Exception()\n",
    "    #img_data = img_data.mean(1, keepdim=True)\n",
    "\n",
    "print(\"mean:\",img_data.shape)\n",
    "\n",
    "\n",
    "cond = model.encode(img_data)\n",
    "print(\"cond\")\n",
    "print(\"cond\",cond.shape)\n",
    "\n",
    "# Decode the encoded representation to obtain the reconstructed 2D image\n",
    "T = 200  # Adjust T as needed\n",
    "stoch = model.encode_stochastic(img_data.to(device), cond, T=T)\n",
    "print(\"stoch shape:\",stoch.shape)\n",
    "xT = model.render(stoch, cond, T=20) #pred = model.render(xT, cond, T=20)\n",
    "reconstructed_img = xT#[0]#(xT[0] + 1) / 2  # De-normalize if needed\n",
    "print(\"shape:\",reconstructed_img.shape)\n",
    "# Move the reconstructed_img to CPU\n",
    "# Set values smaller than 0.1111 to 0, and others to 1\n",
    "reconstructed_img = reconstructed_img.cpu()\n",
    "#reconstructed_img = torch.where(reconstructed_img < 0.11111, torch.tensor(0.0), torch.tensor(1.0))\n",
    "print(np.unique(reconstructed_img))\n",
    "\n",
    "#reconstructed_img = torch.clamp(reconstructed_img, min=0)\n",
    "#print(np.unique(img_slice))\n",
    "\n",
    "r = reconstructed_img[0][0].numpy().copy()\n",
    "reconstructed_img = (r+1)/2\n",
    "print(np.unique(reconstructed_img))\n",
    "reconstructed_img = np.round(reconstructed_img)\n",
    "print(np.unique(reconstructed_img))\n",
    "\n",
    "r = reconstructed_img.copy()\n",
    "\n",
    "i = img_data[0][0].cpu().numpy().copy()\n",
    "i = model_map_to_segmentation_map(i)\n",
    "print(r.shape)\n",
    "#r = r*9\n",
    "#r = np.round(r)\n",
    "re = r#+ 40\n",
    "#print(np.unique(re))\n",
    "#re = np_map_labels(re, {40: 0})\n",
    "print(np.unique(re))\n",
    "nii.set_array_(re).save(\"Reconstructions/Reconstructed_ctfu00699_017_corpus_float_T200_T20.nii.gz\")\n",
    "print(np.unique(nii.get_array()))\n",
    "re = np_map_labels(re, {1: 49})\n",
    "print(np_dice(i, re, binary_compare= True))\n",
    "print(np.unique(i))\n",
    "#print(np.unique(reconstructed_img[0][0].numpy()))\n",
    "print(np.unique(re))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dae_cond",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
