{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'd2c_crop' from partially initialized module 'training.data.celeb' (most likely due to a circular import) (/home/daniel/coding/diffae/notebooks/training/data/celeb.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mglioma_public\u001b[39;00m \u001b[39mimport\u001b[39;00m PublicGliomaDataset\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mexperiment\u001b[39;00m \u001b[39mimport\u001b[39;00m LitModel\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mexperiment_classifier\u001b[39;00m \u001b[39mimport\u001b[39;00m ClsModel\n",
      "File \u001b[0;32m~/coding/diffae/training/data/glioma_public.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mchoices\u001b[39;00m \u001b[39mimport\u001b[39;00m TrainMode\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmri_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     MriCrop, calc_center_of_mass, extract_slices_from_volume, pad_and_resize\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m crop_volume\n\u001b[1;32m     22\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPublicGliomaDataset\u001b[39;00m(Dataset):\n\u001b[1;32m     23\u001b[0m     num_classes: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n",
      "File \u001b[0;32m~/coding/diffae/training/data/transforms.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmri_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m pad\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mceleb\u001b[39;00m \u001b[39mimport\u001b[39;00m d2c_crop\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mRandomResizedCropd\u001b[39m(size, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m mon_transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     11\u001b[0m         mon_transforms\u001b[39m.\u001b[39mRandScaleCropd(roi_scale\u001b[39m=\u001b[39m\u001b[39m0.08\u001b[39m,\n\u001b[1;32m     12\u001b[0m                                       max_roi_scale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         mon_transforms\u001b[39m.\u001b[39mResized(spatial_size\u001b[39m=\u001b[39m[size, size, size], \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     17\u001b[0m     ])\n",
      "File \u001b[0;32m~/coding/diffae/notebooks/training/data/celeb.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlmdb\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseLMDB\n\u001b[1;32m     14\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCrop\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, x1, x2, y1, y2):\n",
      "File \u001b[0;32m~/coding/diffae/notebooks/training/data/lmdb.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mceleb\u001b[39;00m \u001b[39mimport\u001b[39;00m d2c_crop\n\u001b[1;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBaseLMDB\u001b[39;00m(Dataset):\n\u001b[1;32m     14\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, path, original_resolution, zfill: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'd2c_crop' from partially initialized module 'training.data.celeb' (most likely due to a circular import) (/home/daniel/coding/diffae/notebooks/training/data/celeb.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from training.data.glioma_public import PublicGliomaDataset\n",
    "\n",
    "from experiment import LitModel\n",
    "from experiment_classifier import ClsModel\n",
    "from mri_utils import *\n",
    "from templates import gliomapublic_autoenc\n",
    "from templates_cls import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEED = 0\n",
    "np.random.seed(SEEED)\n",
    "torch.manual_seed(SEEED)\n",
    "print(f\"seed = {SEEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensor(t, ax, cmap=\"gray\",*args,**kwargs):\n",
    "    return ax.imshow(t.permute(1,2,0).cpu(),cmap=cmap, *args,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "# device = \"cpu\"\n",
    "conf = gliomapublic_autoenc()\n",
    "print(conf.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitModel(conf)\n",
    "state = torch.load(f'{conf.logdir}/last.ckpt', map_location=\"cpu\")\n",
    "model.load_state_dict(state['state_dict'], strict=False)\n",
    "model.ema_model.eval()\n",
    "model.ema_model.to(device)\n",
    "#model.model.eval()\n",
    "#model.model.to(device)\n",
    "print(f\"global_step: {state['global_step']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.num_workers = 2\n",
    "conf.batch_size = 1\n",
    "\n",
    "data = PublicGliomaDataset(conf.data_path,\n",
    "                           img_size=conf.img_size,\n",
    "                           mri_sequences=conf.mri_sequences,\n",
    "                           mri_crop=conf.mri_crop,\n",
    "                           train_mode=conf.train_mode,\n",
    "                           filter_class_labels=True)\n",
    "loader = torch.utils.data.DataLoader(data, batch_size=conf.batch_size, shuffle=False, num_workers=conf.num_workers)                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_reconstruction = True\n",
    "n_steps = 100\n",
    "remaining_classes = list(range(3))\n",
    "while len(remaining_classes) > 0:\n",
    "    b = next(it)\n",
    "    b_cls = b[\"cls_labels\"][0].item()\n",
    "    if b_cls not in remaining_classes:\n",
    "        continue\n",
    "    remaining_classes.remove(b_cls)\n",
    "    print(f\"Class: {b_cls}\")\n",
    "    b_slice = extract_slices_from_volume(b[\"img\"], b[\"com\"])\n",
    "    if do_reconstruction:\n",
    "        print(\"Reconstructing...\")\n",
    "        img = b[\"img\"].to(device)\n",
    "        com = b[\"com\"].to(device)\n",
    "        cond = model.encode(img)\n",
    "        print(\"encoded...\")\n",
    "        xT = model.encode_stochastic(img, cond, T=n_steps)\n",
    "        print(\"encoded stochastic...\")\n",
    "        rec_img = model.render(xT, cond, T=n_steps).detach().cpu()\n",
    "        print(\"rendered...\")\n",
    "        rec_img_slice = extract_slices_from_volume(rec_img, com)\n",
    "\n",
    "        b_slice = torch.cat([b_slice, rec_img_slice], dim=0)\n",
    "\n",
    "\n",
    "    only_axial = True\n",
    "    n_cols = 4 if only_axial else 12 \n",
    "    n_rows= 1+do_reconstruction\n",
    "    print(n_rows, n_cols)\n",
    "    fig, axs = plt.subplots(n_rows, n_cols,squeeze=True, gridspec_kw = {'wspace':0, 'hspace':0})\n",
    "\n",
    "    stride= 3 if only_axial else 1\n",
    "    for img, ax in zip(b_slice[::stride ], axs.flatten()):\n",
    "        # normalize img to [0,1]\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        plot_tensor(img, ax)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    rec_suffix = \"_rec\" if do_reconstruction else \"\"\n",
    "    img_dir = f\"./imgs/axial_slices{rec_suffix}/\"\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "    plt.savefig(f\"{img_dir}/{b_cls}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from itertools import islice    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduction(x:torch.Tensor, d_low:int=2):\n",
    "    # create covariance matrix of the feature vector in the latent space\n",
    "    cov_x = x.T @ x\n",
    "    # compute the first d_low principal components \n",
    "    u, s, v = torch.pca_lowrank(cov_x, q=d_low)\n",
    "    # project latents onto lower dimensional space\n",
    "    x_low = x @ v\n",
    "    return x_low\n",
    "\n",
    "def tsne_reduction(x:torch.Tensor, d_low: int=2):\n",
    "    n_samples = x.size(0)\n",
    "    tsne = TSNE(n_components=d_low, random_state=SEEED,\n",
    "                       perplexity=min(n_samples-1,50.0))\n",
    "    x_np = x.detach().cpu().numpy()\n",
    "    x_low = tsne.fit_transform(x_np,)\n",
    "    return x_low\n",
    "\n",
    "def umap_reduction(x:torch.Tensor, d_low: int=2):\n",
    "    x_np = x.detach().cpu().numpy()\n",
    "    umap = UMAP(n_components=d_low, init='spectral', random_state=SEEED, low_memory=False,)\n",
    "    x_low = umap.fit_transform(x)\n",
    "    return  x_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init all lists to store the latents in\n",
    "try:\n",
    "    latents_list = [torch.load(f\"{conf.logdir}/latents.pt\")]\n",
    "    cls_labels_list = [torch.load( f\"{conf.logdir}/cls_labels.pt\")]\n",
    "except OSError as e:\n",
    "    print(e)\n",
    "    latents_list = []\n",
    "    cls_labels_list = []\n",
    "i_batch =0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conf.num_workers = 4\n",
    "conf.batch_size =16\n",
    "\n",
    "data = PublicGliomaDataset(conf.data_path,\n",
    "                           img_size=conf.img_size,\n",
    "                           mri_sequences=conf.mri_sequences,\n",
    "                           mri_crop=conf.mri_crop,\n",
    "                           train_mode=conf.train_mode,\n",
    "                           filter_class_labels=True)\n",
    "loader = torch.utils.data.DataLoader(data, batch_size=conf.batch_size, shuffle=False, num_workers=conf.num_workers)                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataloader\n",
    "use_all_data = True\n",
    "if use_all_data:\n",
    "    n = len(loader)\n",
    "else:\n",
    "    n_sample_pts = 10\n",
    "    n = max(1,n_sample_pts // conf.batch_size)\n",
    "print(f\"{n = }\")\n",
    "loader_n = islice(loader, n)\n",
    "batch_iter = tqdm.notebook.tqdm(loader_n,total=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consume data iterator\n",
    "for batch in loader_n:\n",
    "    print(f\"{i_batch = }/{n = }\")\n",
    "    \n",
    "    imgs = batch[\"img\"]\n",
    "    cls_labels_list.append(batch[\"cls_labels\"])\n",
    "    with torch.no_grad():\n",
    "        latent = model.encode(imgs.to(device))\n",
    "    latents_list.append(latent.detach().cpu())\n",
    "    i_batch += 1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = torch.cat(latents_list, dim=0)   \n",
    "cls_labels = torch.cat(cls_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(latents,f\"{conf.logdir}/latents.pt\")\n",
    "torch.save(cls_labels,  f\"{conf.logdir}/cls_labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_red_fns = {\n",
    "    \"pca\": pca_reduction,\n",
    "    \"tsne\": tsne_reduction,\n",
    "    \"umap\": umap_reduction\n",
    "}\n",
    "dim_red_fn_name = \"pca\"\n",
    "for dim_red_fn_name in dim_red_fns.keys():\n",
    "    sample_mask = cls_labels != 1\n",
    "    sample_mask = torch.ones_like(sample_mask, dtype=torch.bool)\n",
    "    latents_low = dim_red_fns[dim_red_fn_name](latents[sample_mask], 2)\n",
    "    cls_labels_low = cls_labels[sample_mask]\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(f\"{dim_red_fn_name}\")\n",
    "    one_vs_all_colors = cls_labels_low == 2\n",
    "    cls_names = [\"Astrocytoma\", \"Glioblastoma\", \"Oligodendroglioma\"]\n",
    "\n",
    "    # set color map for figure\n",
    "    cmap = plt.cm.get_cmap('tab10')\n",
    "    cmap.set_under('white')\n",
    "    cmap.set_over('black')\n",
    "\n",
    "    for i in range(3):\n",
    "        ax.scatter(latents_low[:, 1][cls_labels_low == i],\n",
    "                latents_low[:, 0][cls_labels_low == i],\n",
    "                label=cls_names[i],\n",
    "                alpha=0.5)\n",
    "    ax.legend()\n",
    "    img_dir = \"imgs/plots/\"\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    plt.savefig(f\"{img_dir}/{dim_red_fn_name}_reduction.png\",\n",
    "                bbox_inches=\"tight\",\n",
    "                pad_inches=0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering =  DBSCAN(metric='euclidean', min_samples=10, eps=3,n_jobs=-1).fit(latents_low)\n",
    "#clustering =  KMeans(n_clusters=3, n_init=1).fit(latents_low)\n",
    "c_labels = clustering.labels_\n",
    "c_i = 0\n",
    "c_mask = c_labels==c_i\n",
    "fig, (ax1, ax2)= plt.subplots(1,2)\n",
    "ax1.scatter(latents_low[:,1], latents_low[:,0], c=c_labels)\n",
    "ax2.scatter(latents_low[:,1], latents_low[:,0], c=c_mask)\n",
    "plt.show()\n",
    "# stats across a cluster\n",
    "cluster_to_cls_labels = {}\n",
    "for i_cluster in range(c_labels.max()+1):\n",
    "    cluster_to_cls_labels[i_cluster] = dict(zip(*map(lambda x:x.tolist(),cls_labels_low[c_labels == i_cluster].unique(return_counts=True))))\n",
    "cluster_to_cls_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_given_size(a, size):\n",
    "    return np.split(a, np.arange(size,len(a),size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(data, batch_size=1, shuffle=False, num_workers=conf.num_workers)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_iter = enumerate(zip(loader, c_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_b, (b,b_m) = next(filter(lambda x:x[1][1], img_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i_b +1,\"/\", len(loader))\n",
    "if b_m:\n",
    "    b_img = b[\"img\"]\n",
    "    b_com = b[\"com\"]\n",
    "    # b_img[(b[\"seg_labels\"]>0).repeat(1,4,1,1,1)] = 1\n",
    "    b_img = torch.cat([b_img, b[\"seg_labels\"]],dim=1)\n",
    "\n",
    "    img_slice = extract_slices_from_volume(b_img, b_com)\n",
    "\n",
    "    fig, axs =  plt.subplots(img_slice.size(0)//3, 3)\n",
    "    fig.suptitle(f'Label: {b[\"cls_labels\"][0].item()}')\n",
    "    for img,ax in zip(img_slice, axs.flatten()):  \n",
    "        ax.axis(\"off\")\n",
    "        plot_tensor(img,ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"not in cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffae-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f8f14ac2789a61c7d4a4890af4638ffd689245691e5b64d8cd36864be5139b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
