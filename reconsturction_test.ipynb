{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 41 42 43 44 45 46 47 48 49 50]\n",
      "from_im shape: torch.Size([1, 1, 144, 96, 144])\n",
      "tesnor torch.Size([1, 1, 144, 96, 144])\n",
      "Model params: 1.27 M\n",
      "0.7751068201833067\n",
      "\u001b[0m[ ] 'set_array' with different dtype: from uint8 to float32\u001b[0m\u001b[0m\n",
      "\u001b[96m[*] Save Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T20_Dice0.7751068201833067.nii.gz as uint8\u001b[0m\u001b[0m\n",
      "0.9944312283737025\n",
      "\u001b[96m[*] Save Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T100_Dice0.9944312283737025.nii.gz as uint8\u001b[0m\u001b[0m\n",
      "0.9988050621910814\n",
      "\u001b[96m[*] Save Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T200_Dice0.9988050621910814.nii.gz as uint8\u001b[0m\u001b[0m\n",
      "0.9991304347826087\n",
      "\u001b[96m[*] Save Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T250_Dice0.9991304347826087.nii.gz as uint8\u001b[0m\u001b[0m\n",
      "0.9987625613619984\n",
      "\u001b[96m[*] Save Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T500_Dice0.9987625613619984.nii.gz as uint8\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pl_models.DEA import DAE_LitModel\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from BIDS import NII\n",
    "from BIDS.core.np_utils import np_dice\n",
    "import numpy as np\n",
    "from BIDS.core.np_utils import np_map_labels, Label_Map\n",
    "from sklearn.manifold import TSNE\n",
    "from dataloader.datasets.dataset_csv import model_map_to_segmentation_map, segmentation_map_to_model_map \n",
    "#3D\n",
    "def load_nifti(nii):\n",
    "        \n",
    "        from_im = nii.get_seg_array()\n",
    "        #range_print(torch.from_numpy(from_im.astype(float)),41,41)\n",
    "        #print(from_im[143,95,:])\n",
    "        print(np.unique(from_im))\n",
    "        from_im = np_map_labels(arr=from_im,label_map={50: 49})\n",
    "        from_im = from_im.astype(float)\n",
    "        from_im = segmentation_map_to_model_map(from_im)\n",
    "        #from_im = (from_im-0.5)/0.5\n",
    "        # print(from_im[143,95,143])\n",
    "        # print(\"from_im shape:\", from_im.shape)\n",
    "        # print(\"mapped:\",np.unique(from_im))\n",
    "        from_im = torch.from_numpy(from_im).unsqueeze(0).unsqueeze(0)\n",
    "        print(\"from_im shape:\", from_im.shape)\n",
    "        from_im = from_im.to(torch.float32)\n",
    "        \n",
    "        print(\"tesnor\", from_im.shape)\n",
    "        return from_im # , \"index\": target, \"cls_labels\": target}\n",
    "# def map_to_binary(y):\n",
    "#         #y = y.astype(float)\n",
    "#         labelmap = {i: 1 for i in range(41, 50)} #  labelmap = {i: round((i - 40)/9, ndigits=2) for i in range(41, 50)}\n",
    "#         #print(labelmap)\n",
    "#         return np_map_labels(y, labelmap)\n",
    "# def segmentation_map_to_model_map(y):\n",
    "#         y = y.astype(float)\n",
    "#         labelmap = {i: round((i - 40)/9, ndigits=5) for i in range(41, 50)} #  labelmap = {i: round((i - 40)/9, ndigits=2) for i in range(41, 50)}\n",
    "#         print(labelmap)\n",
    "#         return np_map_labels(y, labelmap)\n",
    "# def model_map_to_segmentation_map(x):\n",
    "#     x =(x+1)/2\n",
    "#     x *= 9\n",
    "#     x = np.round(x+40)# round nearest int\n",
    "#    # x = x+40\n",
    "#     x = np_map_labels(x, {40: 0})\n",
    "#     return x\n",
    "\n",
    "# def model_map_to_binary_map(x):\n",
    "#     x = np.round(x)\n",
    "#     return x\n",
    "\n",
    "def range_print(x, lb, ub):\n",
    "    # Define the range\n",
    "    lower_bound = 0.1\n",
    "    upper_bound = 0.12\n",
    "\n",
    "    # Find values and indices within the range\n",
    "    condition = (x >= lb) & (x <= ub)\n",
    "    values_in_range = x[condition]\n",
    "    indices_in_range = torch.nonzero(condition)\n",
    "\n",
    "    # Print the values and indices\n",
    "    print(\"Values in the range:\", values_in_range)\n",
    "    print(\"Indices in the range:\", indices_in_range)\n",
    "#\"/media/DATA/martina_ma/cutout/verse040/verse040_023_subreg_cropped.nii.gz\",#\n",
    "#/media/DATA/martina_ma/cutout/tri177/tri177_007_subreg_cropped.nii.gz \n",
    "#in training set: /media/DATA/martina_ma/cutout/verse040/verse040_018_subreg_cropped.nii.gz\n",
    "#not in train and not in val: /media/DATA/martina_ma/cutout/verse040/verse040_023_subreg_cropped.nii.gz\n",
    "# Test sample /media/DATA/martina_ma/cutout/tri050/tri050_021_subreg_cropped.nii.gz\n",
    "# Train sample in old seg /media/DATA/martina_ma/cutout/verse529/verse529_014_new_subreg_cropped.nii.gz\n",
    "#Train test    /media/DATA/martina_ma/cutout_clean/ctfu00847/ctfu00847_13_subreg_cropped_cleaned.nii.gz\n",
    "    #/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=57-step=440775_latest.ckpt\n",
    "# fracture train sample /media/DATA/martina_ma/cutout_clean/fxclass0122/fxclass0122_24_subreg_cropped_cleaned.nii.gz\n",
    "very_fractured = \"/media/DATA/martina_ma/cutout_clean/fxclass0149/fxclass0149_13_subreg_cropped_cleaned.nii.gz\"\n",
    "nii = NII.load(\n",
    "    \"/media/DATA/martina_ma/cutout_clean/ctfu00847/ctfu00847_13_subreg_cropped_cleaned.nii.gz\",\n",
    "    True,\n",
    ")\n",
    "from_im = load_nifti(nii)\n",
    "checkpoint_path =\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=1-step=15230d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=0-step=7615d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=2-step=22845d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=57-step=440775_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned/version_5/checkpoints/epoch=11-step=113760d_score=0.9303_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_95_old_verse_w_norm/version_11/checkpoints/epoch=0-step=1488_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_232/checkpoints/epoch=283-step=27190_latest.ckpt\"# \"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_133/checkpoints/epoch=3-step=310_latest.ckpt\"\n",
    "#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_135/checkpoints/epoch=27-step=2775_latest.ckpt\"#/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_133/checkpoints/epoch=3-step=310_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_118/checkpoints/epoch=80-step=243_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_116/checkpoints/epoch=81-step=246_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_108/checkpoints/epoch=47-step=144_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_105/checkpoints/epoch=69-step=210_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_63/checkpoints/epoch=79-step=240_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_60/checkpoints/epoch=71-step=216_latest.ckpt\"#lightning_logs/DAE_NAKO_256/version_26/checkpoints/epoch=404-step=405_latest.ckpt\"\n",
    "device = \"cuda:0\"  # or \"cpu\" if you want to use CPU\n",
    "assert Path(checkpoint_path).exists()\n",
    "model = DAE_LitModel.load_from_checkpoint(checkpoint_path)\n",
    "model.ema_model.eval()\n",
    "model.ema_model.to(device)\n",
    "\n",
    "img_data = from_im.to(device)\n",
    "# Ensure that the input has a single channel (if the model expects 1 channel)\n",
    "if img_data.shape[0] != 1:\n",
    "    raise Exception()\n",
    "\n",
    "cond = model.encode(img_data)\n",
    "\n",
    "# Decode the encoded representation to obtain the reconstructed 2D image\n",
    "timesteps = [20,100,200,250,500]#20,100,200,250,500,\n",
    "for t in timesteps:\n",
    "    stoch = model.encode_stochastic(img_data.to(device), cond, T=t)\n",
    "    xT = model.render(stoch, cond, T=t) \n",
    "    #reconstructed_img = xT.copy()\n",
    "    reconstructed_img = xT.cpu()\n",
    "    r = reconstructed_img[0][0].numpy().copy()\n",
    "    reconstructed_img = model_map_to_segmentation_map(r)\n",
    "    r = reconstructed_img.copy()\n",
    "    i = img_data[0][0].cpu().numpy().copy()\n",
    "    i = model_map_to_segmentation_map(i)\n",
    "    #print(np.unique(r))\n",
    "    dice = np_dice(i, r, binary_compare= True)\n",
    "    print(dice)\n",
    "    nii.set_array_(r).save(\"Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T{}_Dice{}.nii.gz\".format(t,dice))\n",
    "\n",
    "    #{}/{}_{:03d}_subreg_cropped.nii.gz\".format(s[0],subject, label)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 1.27 M\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 23.65 GiB total capacity; 2.68 GiB already allocated; 23.44 MiB free; 2.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/media/DATA/martina_ma/dae/reconsturction_test.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.39.166.52/media/DATA/martina_ma/dae/reconsturction_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m img \u001b[39m=\u001b[39m t[\u001b[39m'\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.39.166.52/media/DATA/martina_ma/dae/reconsturction_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m cond \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(img)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B141.39.166.52/media/DATA/martina_ma/dae/reconsturction_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m stoch \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode_stochastic(img, cond, T\u001b[39m=\u001b[39;49mT_Enc)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.39.166.52/media/DATA/martina_ma/dae/reconsturction_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m xT \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mrender(stoch, cond, T\u001b[39m=\u001b[39mT_Dec)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.39.166.52/media/DATA/martina_ma/dae/reconsturction_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mif\u001b[39;00m arguments\u001b[39m.\u001b[39mDataSet_Option\u001b[39m.\u001b[39mcorpus:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.39.166.52/media/DATA/martina_ma/dae/reconsturction_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39m#xT_mapped = (xT[0][0].cpu().numpy()+1)/2\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.39.166.52/media/DATA/martina_ma/dae/reconsturction_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39m#i = (imgs[0][0].cpu().numpy()+1)/2\u001b[39;00m\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/pl_models/DEA.py:598\u001b[0m, in \u001b[0;36mDAE_LitModel.encode_stochastic\u001b[0;34m(self, x, cond, T)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    596\u001b[0m     sampler \u001b[39m=\u001b[39m get_sampler(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconf, \u001b[39meval\u001b[39m\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, T\u001b[39m=\u001b[39mT)\n\u001b[0;32m--> 598\u001b[0m out \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49mddim_reverse_sample_loop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mema_model, x, model_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mcond\u001b[39;49m\u001b[39m\"\u001b[39;49m: cond})\n\u001b[1;32m    599\u001b[0m \u001b[39mreturn\u001b[39;00m out[\u001b[39m\"\u001b[39m\u001b[39msample\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/diffusion/ddim_sampler.py:627\u001b[0m, in \u001b[0;36mGaussian_Diffusion.ddim_reverse_sample_loop\u001b[0;34m(self, model, x, clip_denoised, denoised_fn, model_kwargs, eta, device)\u001b[0m\n\u001b[1;32m    625\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([i] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(sample), device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    626\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 627\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mddim_reverse_sample(\n\u001b[1;32m    628\u001b[0m         model, sample, t\u001b[39m=\u001b[39;49mt, clip_denoised\u001b[39m=\u001b[39;49mclip_denoised, denoised_fn\u001b[39m=\u001b[39;49mdenoised_fn, model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs, eta\u001b[39m=\u001b[39;49meta\n\u001b[1;32m    629\u001b[0m     )\n\u001b[1;32m    630\u001b[0m     sample \u001b[39m=\u001b[39m out[\u001b[39m\"\u001b[39m\u001b[39msample\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    631\u001b[0m     \u001b[39m# [1, ..., T]\u001b[39;00m\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/diffusion/ddim_sampler.py:586\u001b[0m, in \u001b[0;36mGaussian_Diffusion.ddim_reverse_sample\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs, eta)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[39mSample x_{t+1} from the model using DDIM reverse ODE.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[39mNOTE: never used ?\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[39massert\u001b[39;00m eta \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mReverse ODE only for deterministic path\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 586\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_mean_variance(\n\u001b[1;32m    587\u001b[0m     model,\n\u001b[1;32m    588\u001b[0m     x,\n\u001b[1;32m    589\u001b[0m     t,\n\u001b[1;32m    590\u001b[0m     clip_denoised\u001b[39m=\u001b[39;49mclip_denoised,\n\u001b[1;32m    591\u001b[0m     denoised_fn\u001b[39m=\u001b[39;49mdenoised_fn,\n\u001b[1;32m    592\u001b[0m     model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    594\u001b[0m \u001b[39m# Usually our model outputs epsilon, but we re-derive it\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[39m# in case we used x_start or x_prev prediction.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m eps \u001b[39m=\u001b[39m (_extract_into_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msqrt_recip_alphas_cumprod, t, x\u001b[39m.\u001b[39mshape) \u001b[39m*\u001b[39m x \u001b[39m-\u001b[39m out[\u001b[39m\"\u001b[39m\u001b[39mpred_xstart\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m/\u001b[39m _extract_into_tensor(\n\u001b[1;32m    597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msqrt_recipm1_alphas_cumprod, t, x\u001b[39m.\u001b[39mshape\n\u001b[1;32m    598\u001b[0m )\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/diffusion/ddim_sampler.py:1020\u001b[0m, in \u001b[0;36mSpacedDiffusion.p_mean_variance\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_mean_variance\u001b[39m(\u001b[39mself\u001b[39m, model: Model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):  \u001b[39m# pylint: disable=signature-differs\u001b[39;00m\n\u001b[0;32m-> 1020\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mp_mean_variance(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_model(model), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/diffusion/ddim_sampler.py:276\u001b[0m, in \u001b[0;36mGaussian_Diffusion.p_mean_variance\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39massert\u001b[39;00m t\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (B,)\n\u001b[1;32m    275\u001b[0m \u001b[39mwith\u001b[39;00m autocast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16):\n\u001b[0;32m--> 276\u001b[0m     model_forward \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(x\u001b[39m=\u001b[39;49mx, t\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scale_timesteps(t), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    277\u001b[0m model_output \u001b[39m=\u001b[39m model_forward\u001b[39m.\u001b[39mpred\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_var_type \u001b[39min\u001b[39;00m [ModelVarType\u001b[39m.\u001b[39mfixed_large, ModelVarType\u001b[39m.\u001b[39mfixed_small]:\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/diffusion/ddim_sampler.py:1096\u001b[0m, in \u001b[0;36m_WrappedModel.forward\u001b[0;34m(self, x, t, t_cond, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mif\u001b[39;00m t_cond \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# support t_cond\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     t_cond \u001b[39m=\u001b[39m do(t_cond)\n\u001b[0;32m-> 1096\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x\u001b[39m=\u001b[39;49mx, t\u001b[39m=\u001b[39;49mdo(t), t_cond\u001b[39m=\u001b[39;49mt_cond, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/dae_cond/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/models/unet_with_encoder.py:298\u001b[0m, in \u001b[0;36mDiffusion_Autoencoder_Model.forward\u001b[0;34m(self, x, t, y, x_start, cond, style, noise, t_cond, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m             lateral \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    296\u001b[0m             \u001b[39m# print(i, j, lateral)\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m         h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_blocks[k](h, emb\u001b[39m=\u001b[39;49mdec_time_emb, cond\u001b[39m=\u001b[39;49mdec_cond_emb, lateral\u001b[39m=\u001b[39;49mlateral)\n\u001b[1;32m    299\u001b[0m         k \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    301\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(h)\n",
      "File \u001b[0;32m~/.conda/envs/dae_cond/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/models/blocks.py:40\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb, cond, lateral)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m     39\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, TimestepBlock):\n\u001b[0;32m---> 40\u001b[0m         x \u001b[39m=\u001b[39m layer(x, emb\u001b[39m=\u001b[39;49memb, cond\u001b[39m=\u001b[39;49mcond, lateral\u001b[39m=\u001b[39;49mlateral)\n\u001b[1;32m     41\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m~/.conda/envs/dae_cond/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/models/blocks.py:183\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[0;34m(self, x, emb, cond, lateral)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, emb\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cond\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lateral\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    176\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m    Apply the block to a Tensor, conditioned on a timestep embedding.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39m        lateral: lateral connection from the encoder\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_checkpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward, (x, emb, cond, lateral), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconf\u001b[39m.\u001b[39;49muse_checkpoint)\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/models/nn.py:146\u001b[0m, in \u001b[0;36mtorch_checkpoint\u001b[0;34m(func, args, flag, preserve_rng_state)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    144\u001b[0m         func, \u001b[39m*\u001b[39margs, preserve_rng_state\u001b[39m=\u001b[39mpreserve_rng_state)\n\u001b[1;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/models/blocks.py:209\u001b[0m, in \u001b[0;36mResBlock._forward\u001b[0;34m(self, x, emb, cond, lateral)\u001b[0m\n\u001b[1;32m    207\u001b[0m     h \u001b[39m=\u001b[39m in_conv(h)\n\u001b[1;32m    208\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_layers(x)\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconf\u001b[39m.\u001b[39muse_condition:\n\u001b[1;32m    212\u001b[0m     \u001b[39m# it's possible that the network may not receieve the time emb\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[39m# this happens with autoenc and setting the time_at\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39mif\u001b[39;00m emb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/dae_cond/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/dae_cond/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/dae_cond/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/DATA/martina_ma/dae/models/nn.py:21\u001b[0m, in \u001b[0;36mGroupNorm32.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(x\u001b[39m.\u001b[39;49mfloat())\u001b[39m.\u001b[39mtype(x\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.conda/envs/dae_cond/lib/python3.10/site-packages/torch/nn/modules/normalization.py:272\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 272\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mgroup_norm(\n\u001b[1;32m    273\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_groups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/.conda/envs/dae_cond/lib/python3.10/site-packages/torch/nn/functional.py:2516\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2514\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(group_norm, (\u001b[39minput\u001b[39m, weight, bias,), \u001b[39minput\u001b[39m, num_groups, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps)\n\u001b[1;32m   2515\u001b[0m _verify_batch_size([\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_groups, num_groups] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()[\u001b[39m2\u001b[39m:]))\n\u001b[0;32m-> 2516\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgroup_norm(\u001b[39minput\u001b[39;49m, num_groups, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 23.65 GiB total capacity; 2.68 GiB already allocated; 23.44 MiB free; 2.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from scipy import ndimage\n",
    "from BIDS import BIDS_Global_info, NII\n",
    "from BIDS.core.np_utils import np_calc_crop_around_centerpoint, np_bbox_nd\n",
    "#from BIDS import NII\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "#from BIDS.core.poi import load_poi, VertebraCentroids\n",
    "from IPython.display import Image\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from pl_models.DEA import DAE_LitModel\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from torchvision import transforms\n",
    "from BIDS import NII\n",
    "from BIDS.core.np_utils import np_dice\n",
    "import numpy as np\n",
    "from BIDS.core.np_utils import np_map_labels, Label_Map\n",
    "from sklearn.manifold import TSNE\n",
    "import torchvision\n",
    "from dataloader.dataset_factory import get_data_loader, get_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import arguments\n",
    "import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "from BIDS.core import vert_constants\n",
    "from BIDS.core.np_utils import np_volume\n",
    "from dataloader.datasets.dataset_csv import model_map_to_segmentation_map, segmentation_map_to_model_map, model_map_to_corpus \n",
    "#from utils.metadata import make_anomaly_dict, add_fxclass_fracture_anomaly, parse_string\n",
    "import utils.metadata as m\n",
    "from BIDS.core.vert_constants import v_name2idx, v_idx2name\n",
    "\n",
    "arguments.DataSet_Option.corpus = True\n",
    "arguments.DataSet_Option.dataset = \"/media/DATA/martina_ma/dae/test_set_corpus_filtered_cleaned.csv\"\n",
    "dataset = get_dataset(arguments.DataSet_Option,\"test\")\n",
    "checkpoint_path =\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_corpus/version_20/checkpoints/epoch=15-step=121840d_score=1.0000_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=1-step=15230d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=0-step=7615d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=2-step=22845d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=57-step=440775_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned/version_5/checkpoints/epoch=11-step=113760d_score=0.9303_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_95_old_verse_w_norm/version_11/checkpoints/epoch=0-step=1488_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_232/checkpoints/epoch=283-step=27190_latest.ckpt\"# \"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_133/checkpoints/epoch=3-step=310_latest.ckpt\"\n",
    "#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_135/checkpoints/epoch=27-step=2775_latest.ckpt\"#/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_133/checkpoints/epoch=3-step=310_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_118/checkpoints/epoch=80-step=243_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_116/checkpoints/epoch=81-step=246_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_108/checkpoints/epoch=47-step=144_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_105/checkpoints/epoch=69-step=210_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_63/checkpoints/epoch=79-step=240_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_60/checkpoints/epoch=71-step=216_latest.ckpt\"#lightning_logs/DAE_NAKO_256/version_26/checkpoints/epoch=404-step=405_latest.ckpt\"\n",
    "device = \"cuda:0\"\n",
    "assert Path(checkpoint_path).exists()\n",
    "model = DAE_LitModel.load_from_checkpoint(checkpoint_path)\n",
    "model.ema_model.eval()\n",
    "model.ema_model.to(device)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,#opt.batch_size,\n",
    "        #sampler=sampler,\n",
    "        # with sampler, use the sample instead of this option\n",
    "        shuffle=False,# if sampler else shuffle,\n",
    "        num_workers=16,#opt.num_cpu,\n",
    "        pin_memory=True,\n",
    "        #drop_last=drop_last,\n",
    "        #multiprocessing_context=get_context(\"fork\"),\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "dice_scores_test = []\n",
    "T_Enc = 200\n",
    "T_Dec = 20\n",
    "for t in test_dataloader:\n",
    "    img = t['img'].to(device)\n",
    "    cond = model.encode(img)\n",
    "    stoch = model.encode_stochastic(img, cond, T=T_Enc)\n",
    "    xT = model.render(stoch, cond, T=T_Dec)\n",
    "    if arguments.DataSet_Option.corpus:\n",
    "        #xT_mapped = (xT[0][0].cpu().numpy()+1)/2\n",
    "        #i = (imgs[0][0].cpu().numpy()+1)/2\n",
    "        xT_mapped = model_map_to_corpus(xT[0][0].cpu().numpy())\n",
    "        i = model_map_to_corpus(img[0][0].cpu().numpy())\n",
    "        l_x = np.unique(xT_mapped)\n",
    "        l_i = np.unique(i)\n",
    "        d = dice(torch.tensor(xT_mapped,dtype=int),torch.tensor(i,dtype=int),ignore_index= 0)#, multiclass=True)\n",
    "        print(d)\n",
    "    else:\n",
    "        xT_mapped = model_map_to_segmentation_map(xT[0][0].cpu().numpy())\n",
    "        xT_mapped = xT_mapped-40\n",
    "        xT_mapped = np_map_labels(xT_mapped, {-40: 0})\n",
    "        i = model_map_to_segmentation_map(img[0][0].cpu().numpy())\n",
    "        i = i-40\n",
    "        i = np_map_labels(i, {-40: 0})\n",
    "        d = dice(torch.tensor(xT_mapped,dtype=int),torch.tensor(i,dtype=int), average = 'macro', ignore_index= 0, num_classes=10)#, multiclass=True)\n",
    "        print(d)\n",
    "    dice_scores_test.append(d)\n",
    "\n",
    "reconstruction = {'TimestepsEnc':T_Enc, 'TimestepsDec':T_Dec, \"ReconstDice\": dice_scores_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200  # Adjust T as needed\n",
    "stoch = model.encode_stochastic(img_data.to(device), cond, T=T)\n",
    "print(\"stoch shape:\",stoch.shape)\n",
    "xT = model.render(stoch, cond, T=T) #pred = model.render(xT, cond, T=20)\n",
    "reconstructed_img = xT#[0]#(xT[0] + 1) / 2  # De-normalize if needed\n",
    "print(\"shape:\",reconstructed_img.shape)\n",
    "# Move the reconstructed_img to CPU\n",
    "# Set values smaller than 0.1111 to 0, and others to 1\n",
    "reconstructed_img = reconstructed_img.cpu()\n",
    "#reconstructed_img = torch.where(reconstructed_img < 0.11111, torch.tensor(0.0), torch.tensor(1.0))\n",
    "print(np.unique(reconstructed_img))\n",
    "print(reconstructed_img[0][0][143,95,143])\n",
    "#reconstructed_img = torch.clamp(reconstructed_img, min=0)\n",
    "#print(np.unique(img_slice))\n",
    "\n",
    "#reconstructed_img = torch.where(reconstructed_img < 0, torch.tensor(0.0), reconstructed_img)\n",
    "# print(\"re_z:\",reconstructed_img[0][0][0,0,:])\n",
    "# print(\"re_y:\",reconstructed_img[0][0][0,:,0])\n",
    "# print(\"re_x:\",reconstructed_img[0][0][:,0,0])\n",
    "r = reconstructed_img[0][0].numpy().copy()\n",
    "reconstructed_img = model_map_to_segmentation_map(r)\n",
    "# print(reconstructed_img[0][0][143,95,143])\n",
    "# #print(np.unique(img_slice))\n",
    "# #\n",
    "# print(reconstructed_img[0].shape)\n",
    "# print(np.unique(reconstructed_img))\n",
    "# print(\"z:\",reconstructed_img[0][0][0,0,:])\n",
    "# print(\"y:\",reconstructed_img[0][0][0,:,0])\n",
    "# print(\"x:\",reconstructed_img[0][0][:,0,0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "from BIDS import BIDS_Global_info, NII\n",
    "from BIDS.core.np_utils import np_calc_crop_around_centerpoint, np_bbox_nd\n",
    "#from BIDS import NII\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "#from BIDS.core.poi import load_poi, VertebraCentroids\n",
    "from IPython.display import Image\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from pl_models.DEA import DAE_LitModel\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from torchvision import transforms\n",
    "from BIDS import NII\n",
    "from BIDS.core.np_utils import np_dice\n",
    "import numpy as np\n",
    "from BIDS.core.np_utils import np_map_labels, Label_Map\n",
    "from sklearn.manifold import TSNE\n",
    "import torchvision\n",
    "from dataloader.dataset_factory import get_data_loader, get_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import arguments\n",
    "import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "from BIDS.core import vert_constants\n",
    "from BIDS.core.np_utils import np_volume\n",
    "from dataloader.datasets.dataset_csv import model_map_to_segmentation_map, segmentation_map_to_model_map, model_map_to_corpus \n",
    "#from utils.metadata import make_anomaly_dict, add_fxclass_fracture_anomaly, parse_string\n",
    "import utils.metadata as m\n",
    "from BIDS.core.vert_constants import v_name2idx, v_idx2name\n",
    "\n",
    "arguments.DataSet_Option.dataset = \"/media/DATA/martina_ma/dae/test_set_cleaned_and_not_filtered.csv\"\n",
    "dataset = get_dataset(arguments.DataSet_Option,\"test\")\n",
    "test_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,#opt.batch_size,\n",
    "        #sampler=sampler,\n",
    "        # with sampler, use the sample instead of this option\n",
    "        shuffle=False,# if sampler else shuffle,\n",
    "        num_workers=16,#opt.num_cpu,\n",
    "        pin_memory=True,\n",
    "        #drop_last=drop_last,\n",
    "        #multiprocessing_context=get_context(\"fork\"),\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "#/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=57-step=435130_latest.ckpt\n",
    "checkpoint_path =\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_corpus/version_20/checkpoints/epoch=5-step=45690d_score=1.0000_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_corpus/version_20/checkpoints/epoch=15-step=121840d_score=1.0000_d_score_latest.ckpt\"#/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_corpus/version_20/checkpoints/epoch=25-step=197990d_score=1.0000_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=1-step=15230d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=2-step=22845d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=57-step=435130_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=0-step=7615d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_95_old_verse_w_norm/version_30/checkpoints/epoch=10-step=46717d_score=0.9912_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_133/checkpoints/epoch=3-step=310_latest.ckpt\"#/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_95_old_verse_w_norm/version_10/checkpoints/epoch=0-step=1488_d_score_latest copy.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_232/checkpoints/epoch=283-step=27190_latest.ckpt\"# \"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_133/checkpoints/epoch=3-step=310_latest.ckpt\"\n",
    "device = \"cuda:0\"  # or \"cpu\" if you want to use CPU\n",
    "assert Path(checkpoint_path).exists()\n",
    "model = DAE_LitModel.load_from_checkpoint(checkpoint_path)\n",
    "model.ema_model.eval()\n",
    "model.ema_model.to(device)\n",
    "print(\"epoch5\")\n",
    "embeddings_dataframe = extract_embeddings(train_dataloader, val_dataloader, model)\n",
    "torch.save(embeddings_dataframe,'/media/DATA/martina_ma/emb_dict_3D_corpus_epoch5.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dae_cond",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
