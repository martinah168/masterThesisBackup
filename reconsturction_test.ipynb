{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 41 42 43 44 45 46 47 48 49 50]\n",
      "from_im shape: torch.Size([1, 1, 144, 96, 144])\n",
      "tesnor torch.Size([1, 1, 144, 96, 144])\n",
      "Model params: 1.27 M\n",
      "0.7751068201833067\n",
      "\u001b[0m[ ] 'set_array' with different dtype: from uint8 to float32\u001b[0m\u001b[0m\n",
      "\u001b[96m[*] Save Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T20_Dice0.7751068201833067.nii.gz as uint8\u001b[0m\u001b[0m\n",
      "0.9944312283737025\n",
      "\u001b[96m[*] Save Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T100_Dice0.9944312283737025.nii.gz as uint8\u001b[0m\u001b[0m\n",
      "0.9988050621910814\n",
      "\u001b[96m[*] Save Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T200_Dice0.9988050621910814.nii.gz as uint8\u001b[0m\u001b[0m\n",
      "0.9991304347826087\n",
      "\u001b[96m[*] Save Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T250_Dice0.9991304347826087.nii.gz as uint8\u001b[0m\u001b[0m\n",
      "0.9987625613619984\n",
      "\u001b[96m[*] Save Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T500_Dice0.9987625613619984.nii.gz as uint8\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pl_models.DEA import DAE_LitModel\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from BIDS import NII\n",
    "from BIDS.core.np_utils import np_dice\n",
    "import numpy as np\n",
    "from BIDS.core.np_utils import np_map_labels, Label_Map\n",
    "from sklearn.manifold import TSNE\n",
    "from dataloader.datasets.dataset_csv import model_map_to_segmentation_map, segmentation_map_to_model_map \n",
    "#3D\n",
    "def load_nifti(nii):\n",
    "        \n",
    "        from_im = nii.get_seg_array()\n",
    "        #range_print(torch.from_numpy(from_im.astype(float)),41,41)\n",
    "        #print(from_im[143,95,:])\n",
    "        print(np.unique(from_im))\n",
    "        from_im = np_map_labels(arr=from_im,label_map={50: 49})\n",
    "        from_im = from_im.astype(float)\n",
    "        from_im = segmentation_map_to_model_map(from_im)\n",
    "        #from_im = (from_im-0.5)/0.5\n",
    "        # print(from_im[143,95,143])\n",
    "        # print(\"from_im shape:\", from_im.shape)\n",
    "        # print(\"mapped:\",np.unique(from_im))\n",
    "        from_im = torch.from_numpy(from_im).unsqueeze(0).unsqueeze(0)\n",
    "        print(\"from_im shape:\", from_im.shape)\n",
    "        from_im = from_im.to(torch.float32)\n",
    "        \n",
    "        print(\"tesnor\", from_im.shape)\n",
    "        return from_im # , \"index\": target, \"cls_labels\": target}\n",
    "# def map_to_binary(y):\n",
    "#         #y = y.astype(float)\n",
    "#         labelmap = {i: 1 for i in range(41, 50)} #  labelmap = {i: round((i - 40)/9, ndigits=2) for i in range(41, 50)}\n",
    "#         #print(labelmap)\n",
    "#         return np_map_labels(y, labelmap)\n",
    "# def segmentation_map_to_model_map(y):\n",
    "#         y = y.astype(float)\n",
    "#         labelmap = {i: round((i - 40)/9, ndigits=5) for i in range(41, 50)} #  labelmap = {i: round((i - 40)/9, ndigits=2) for i in range(41, 50)}\n",
    "#         print(labelmap)\n",
    "#         return np_map_labels(y, labelmap)\n",
    "# def model_map_to_segmentation_map(x):\n",
    "#     x =(x+1)/2\n",
    "#     x *= 9\n",
    "#     x = np.round(x+40)# round nearest int\n",
    "#    # x = x+40\n",
    "#     x = np_map_labels(x, {40: 0})\n",
    "#     return x\n",
    "\n",
    "# def model_map_to_binary_map(x):\n",
    "#     x = np.round(x)\n",
    "#     return x\n",
    "\n",
    "def range_print(x, lb, ub):\n",
    "    # Define the range\n",
    "    lower_bound = 0.1\n",
    "    upper_bound = 0.12\n",
    "\n",
    "    # Find values and indices within the range\n",
    "    condition = (x >= lb) & (x <= ub)\n",
    "    values_in_range = x[condition]\n",
    "    indices_in_range = torch.nonzero(condition)\n",
    "\n",
    "    # Print the values and indices\n",
    "    print(\"Values in the range:\", values_in_range)\n",
    "    print(\"Indices in the range:\", indices_in_range)\n",
    "#\"/media/DATA/martina_ma/cutout/verse040/verse040_023_subreg_cropped.nii.gz\",#\n",
    "#/media/DATA/martina_ma/cutout/tri177/tri177_007_subreg_cropped.nii.gz \n",
    "#in training set: /media/DATA/martina_ma/cutout/verse040/verse040_018_subreg_cropped.nii.gz\n",
    "#not in train and not in val: /media/DATA/martina_ma/cutout/verse040/verse040_023_subreg_cropped.nii.gz\n",
    "# Test sample /media/DATA/martina_ma/cutout/tri050/tri050_021_subreg_cropped.nii.gz\n",
    "# Train sample in old seg /media/DATA/martina_ma/cutout/verse529/verse529_014_new_subreg_cropped.nii.gz\n",
    "#Train test    /media/DATA/martina_ma/cutout_clean/ctfu00847/ctfu00847_13_subreg_cropped_cleaned.nii.gz\n",
    "    #/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=57-step=440775_latest.ckpt\n",
    "# fracture train sample /media/DATA/martina_ma/cutout_clean/fxclass0122/fxclass0122_24_subreg_cropped_cleaned.nii.gz\n",
    "very_fractured = \"/media/DATA/martina_ma/cutout_clean/fxclass0149/fxclass0149_13_subreg_cropped_cleaned.nii.gz\"\n",
    "nii = NII.load(\n",
    "    \"/media/DATA/martina_ma/cutout_clean/ctfu00847/ctfu00847_13_subreg_cropped_cleaned.nii.gz\",\n",
    "    True,\n",
    ")\n",
    "from_im = load_nifti(nii)\n",
    "checkpoint_path =\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=1-step=15230d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=0-step=7615d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=2-step=22845d_score=0.9726_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned_balanced/version_11/checkpoints/epoch=57-step=440775_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_cleaned/version_5/checkpoints/epoch=11-step=113760d_score=0.9303_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_3D_95_old_verse_w_norm/version_11/checkpoints/epoch=0-step=1488_d_score_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_232/checkpoints/epoch=283-step=27190_latest.ckpt\"# \"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_133/checkpoints/epoch=3-step=310_latest.ckpt\"\n",
    "#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_135/checkpoints/epoch=27-step=2775_latest.ckpt\"#/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_133/checkpoints/epoch=3-step=310_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_118/checkpoints/epoch=80-step=243_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_116/checkpoints/epoch=81-step=246_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_108/checkpoints/epoch=47-step=144_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_105/checkpoints/epoch=69-step=210_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_63/checkpoints/epoch=79-step=240_latest.ckpt\"#\"/media/DATA/martina_ma/dae/lightning_logs/DAE_NAKO_256/version_60/checkpoints/epoch=71-step=216_latest.ckpt\"#lightning_logs/DAE_NAKO_256/version_26/checkpoints/epoch=404-step=405_latest.ckpt\"\n",
    "device = \"cuda:0\"  # or \"cpu\" if you want to use CPU\n",
    "assert Path(checkpoint_path).exists()\n",
    "model = DAE_LitModel.load_from_checkpoint(checkpoint_path)\n",
    "model.ema_model.eval()\n",
    "model.ema_model.to(device)\n",
    "\n",
    "img_data = from_im.to(device)\n",
    "# Ensure that the input has a single channel (if the model expects 1 channel)\n",
    "if img_data.shape[0] != 1:\n",
    "    raise Exception()\n",
    "\n",
    "cond = model.encode(img_data)\n",
    "\n",
    "# Decode the encoded representation to obtain the reconstructed 2D image\n",
    "timesteps = [20,100,200,250,500]#20,100,200,250,500,\n",
    "for t in timesteps:\n",
    "    stoch = model.encode_stochastic(img_data.to(device), cond, T=t)\n",
    "    xT = model.render(stoch, cond, T=t) \n",
    "    #reconstructed_img = xT.copy()\n",
    "    reconstructed_img = xT.cpu()\n",
    "    r = reconstructed_img[0][0].numpy().copy()\n",
    "    reconstructed_img = model_map_to_segmentation_map(r)\n",
    "    r = reconstructed_img.copy()\n",
    "    i = img_data[0][0].cpu().numpy().copy()\n",
    "    i = model_map_to_segmentation_map(i)\n",
    "    #print(np.unique(r))\n",
    "    dice = np_dice(i, r, binary_compare= True)\n",
    "    print(dice)\n",
    "    nii.set_array_(r).save(\"Reconstructions/ReconstructEpoch1/Epoch1_Reconstructed_ctfu00847_13_T{}_Dice{}.nii.gz\".format(t,dice))\n",
    "\n",
    "    #{}/{}_{:03d}_subreg_cropped.nii.gz\".format(s[0],subject, label)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200  # Adjust T as needed\n",
    "stoch = model.encode_stochastic(img_data.to(device), cond, T=T)\n",
    "print(\"stoch shape:\",stoch.shape)\n",
    "xT = model.render(stoch, cond, T=T) #pred = model.render(xT, cond, T=20)\n",
    "reconstructed_img = xT#[0]#(xT[0] + 1) / 2  # De-normalize if needed\n",
    "print(\"shape:\",reconstructed_img.shape)\n",
    "# Move the reconstructed_img to CPU\n",
    "# Set values smaller than 0.1111 to 0, and others to 1\n",
    "reconstructed_img = reconstructed_img.cpu()\n",
    "#reconstructed_img = torch.where(reconstructed_img < 0.11111, torch.tensor(0.0), torch.tensor(1.0))\n",
    "print(np.unique(reconstructed_img))\n",
    "print(reconstructed_img[0][0][143,95,143])\n",
    "#reconstructed_img = torch.clamp(reconstructed_img, min=0)\n",
    "#print(np.unique(img_slice))\n",
    "\n",
    "#reconstructed_img = torch.where(reconstructed_img < 0, torch.tensor(0.0), reconstructed_img)\n",
    "# print(\"re_z:\",reconstructed_img[0][0][0,0,:])\n",
    "# print(\"re_y:\",reconstructed_img[0][0][0,:,0])\n",
    "# print(\"re_x:\",reconstructed_img[0][0][:,0,0])\n",
    "r = reconstructed_img[0][0].numpy().copy()\n",
    "reconstructed_img = model_map_to_segmentation_map(r)\n",
    "# print(reconstructed_img[0][0][143,95,143])\n",
    "# #print(np.unique(img_slice))\n",
    "# #\n",
    "# print(reconstructed_img[0].shape)\n",
    "# print(np.unique(reconstructed_img))\n",
    "# print(\"z:\",reconstructed_img[0][0][0,0,:])\n",
    "# print(\"y:\",reconstructed_img[0][0][0,:,0])\n",
    "# print(\"x:\",reconstructed_img[0][0][:,0,0])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dae_cond",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
